{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694289e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN code for prediction\n",
    "# method: CNN-BiLSTM\n",
    "# requirements: python3.7 keras2.2.5 tensorflow-cpu1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbbf9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# IMPORTS\n",
    "from keras.layers import Input, Dense, LSTM ,Conv1D,Dropout,Bidirectional,Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from numpy.random import seed\n",
    "from keras.models import Model\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy import vstack,hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8ba9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# FUNCTIONS\n",
    "#--CNN1D\n",
    "def train_cnn(x_tra,y_tra,temp_save_path,iens):\n",
    "    \n",
    "    #set early stopping criteria\n",
    "    pat = 20 #this is the number of epochs with no improvment after which the training will stop\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=0)\n",
    "   \n",
    "    #define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
    "    model_save_name = '{0}cnn_model_ens{1:0>3}.h5'.format(*temp_save_path,iens)\n",
    "    model_checkpoint = ModelCheckpoint(model_save_name, verbose=0, save_best_only=True)\n",
    "   \n",
    "    n_folds=5\n",
    "    #save the model history in a list after fitting so that we can plot later\n",
    "    model_history = []\n",
    "    for i in range(n_folds):\n",
    "        print(\"Training on Fold: \",i+1)\n",
    "        model = my_cnn_model([x_tra.shape[1], 1])\n",
    "        history=model.fit(x_tra, y_tra,epochs=1000, callbacks=[early_stopping,model_checkpoint], \n",
    "                 batch_size=16, shuffle=True, verbose=1, validation_split=0.1)\n",
    "        model_history.append(history)\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        # epochs = range(len(loss))\n",
    "        # plt.figure()\n",
    "        # plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "        # plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
    "        # plt.title('Training and validation loss')\n",
    "        # plt.legend()\n",
    "        np.savez(\"{0}traineval_ens{1:0>3}_fold{2:0>3}.npz\".format(*temp_save_path,iens,i), loss=loss,val_loss=val_loss)\n",
    "       \n",
    "    model = load_model(model_save_name)\n",
    "    return model\n",
    "\n",
    "def my_cnn_model(input_size):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=1, activation='relu', \n",
    "                     input_shape=input_size,kernel_initializer=\"he_uniform\"))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    # model.add(Dense(64, activation='relu'))\n",
    "    # model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    # model.compile(optimizer='adam', loss='mse')\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, \n",
    "                                            beta_2=0.999, epsilon=1e-07, \n",
    "                                            amsgrad=False,decay=1e-06), loss='mse')\n",
    "    # model.compile(optimizer=optimizers.SGD(learning_rate=0.1, momentum=0.1), loss='mse')\n",
    "    # model.compile(optimizer=optimizers.RMSprop(learning_rate=0.01),loss='mse')\n",
    "    return model\n",
    "\n",
    "#--BiLSTM\n",
    "def attention_model(window, dims,lunits):\n",
    "    inputs = Input(shape=(window, dims))\n",
    "\n",
    "    x = Conv1D(filters = 32, kernel_size = 1, activation = 'relu',kernel_initializer=\"he_uniform\")(inputs)  #, padding = 'same'\n",
    "    # x = Dropout(0.7)(x)\n",
    "    #lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "    lstm_out = Bidirectional(LSTM(lunits, return_sequences=True))(x)\n",
    "    lstm_out = Dropout(0.5)(lstm_out)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    \n",
    "    output = Dense(1, activation='linear')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    #a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2), name='attention_vec')(a)\n",
    "\n",
    "    #output_attention_mul = concatenate([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "#--others\n",
    "def create_dataset(dataset, look_back):\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back),:]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back,:])\n",
    "    TrainX = np.array(dataX)\n",
    "    Train_Y = np.array(dataY)\n",
    "\n",
    "    return TrainX, Train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e192484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# MAIN\n",
    "\n",
    "#--define parameters for traning\n",
    "INPUT_DIMS = 1\n",
    "TIME_STEPS = 10\n",
    "lstm_units = 32 #16\n",
    "\n",
    "yrs        = np.arange(1500,1949,step=1)\n",
    "yrf        = np.arange(1950,2100,step=1)\n",
    "nens       = 500 # repeat how many times\n",
    "\n",
    "trend_r2   = np.zeros((nens,2)) #train,test\n",
    "osc_r2     = np.zeros((nens,2)) #train.test\n",
    "trend_mse  = np.zeros((nens,2)) #train,test\n",
    "osc_mse    = np.zeros((nens,2)) #train.test\n",
    "recent_r2  = np.zeros((nens,2)) #1960-2000 2004-2017\n",
    "\n",
    "temp_save_path = [\"./temp_cnn/\"]\n",
    "pdo_save_path  = [\"./pdo_cnn_lstm/\"]\n",
    "\n",
    "#--read in files\n",
    "# historical-model training\n",
    "df_1       = pd.read_csv('historical_imfs.csv',header=None)\n",
    "# df_1       = pd.read_csv('G:/Project2/final-codes/CEEMDAN_V00/hist_imf_hail.txt',header=None)\n",
    "data_1     = df_1.values\n",
    "data_1     = data_1.transpose() # nyear*nvar\n",
    "YY_osc     = data_1[:,0] # target-oscillation\n",
    "YY_trend   = data_1[:,1] # target-trend \n",
    "XX_osc     = data_1[:,2] # features-oscillation\n",
    "XX_trend   = data_1[:,3] # features-trend\n",
    "\n",
    "# future-prediction\n",
    "df_2       = pd.read_csv('future_imfs.txt',header=None)\n",
    "data_2     = df_2.values\n",
    "data_2     = data_2.transpose()\n",
    "XX_osc_fut = data_2[:,0]\n",
    "XX_trend_fut=data_2[:,1:5] # ssp126,245,370,585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45e777e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--preprocessing\n",
    "\n",
    "# training\n",
    "Scaler_Input_trend        = MinMaxScaler(feature_range = (-1, 1) )\n",
    "Input_Train_Scaler_trend  = Scaler_Input_trend.fit_transform(XX_trend.reshape(-1,1))\n",
    "\n",
    "Scaler_Output_trend       = MinMaxScaler(feature_range = (-1, 1) ) \n",
    "Output_Train_Scaler_trend = Scaler_Output_trend.fit_transform(YY_trend.reshape(-1,1))\n",
    "\n",
    "Scaler_Input_osc          = MinMaxScaler(feature_range = (-1, 1) )\n",
    "Input_Train_Scaler_osc    = Scaler_Input_osc.fit_transform(XX_osc.reshape(-1,1))\n",
    "\n",
    "Scaler_Output_osc         = MinMaxScaler(feature_range = (-1, 1) ) \n",
    "Output_Train_Scaler_osc   = Scaler_Output_osc.fit_transform(YY_osc.reshape(-1,1))\n",
    "\n",
    "# forecast\n",
    "Input_Forecast126_Scaler  = Scaler_Input_trend.transform(XX_trend_fut[:,0].reshape(-1,1))\n",
    "Input_Forecast245_Scaler  = Scaler_Input_trend.transform(XX_trend_fut[:,1].reshape(-1,1))\n",
    "Input_Forecast370_Scaler  = Scaler_Input_trend.transform(XX_trend_fut[:,2].reshape(-1,1))\n",
    "Input_Forecast585_Scaler  = Scaler_Input_trend.transform(XX_trend_fut[:,3].reshape(-1,1))\n",
    "\n",
    "Input_Forecast_Scaler     = Scaler_Input_osc.transform(XX_osc_fut.reshape(-1,1))\n",
    "\n",
    "XX1, _    = create_dataset(Input_Train_Scaler_osc, TIME_STEPS)\n",
    "_,YY1     = create_dataset(Output_Train_Scaler_osc, TIME_STEPS)\n",
    "XX1_fut,_ = create_dataset(Input_Forecast_Scaler, TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59297d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--train and prediction\n",
    "np.random.seed(2023)\n",
    "for iens in range(nens):\n",
    "    \n",
    "    # training for oscillation##################################\n",
    "    # train\n",
    "    train_X1, test_X1, train_Y1, test_Y1=train_test_split(XX1, YY1, test_size=0.2)\n",
    "    m1 = attention_model(TIME_STEPS,INPUT_DIMS,lstm_units)\n",
    "    m1.summary()\n",
    "    m1.compile(optimizer=optimizers.Adam(learning_rate=0.001, \n",
    "                                         beta_1=0.9, beta_2=0.999, \n",
    "                                         epsilon=1e-07, amsgrad=False,decay=1e-06), loss='mse')\n",
    "    model_save_name = '{0}lstm_model_ens{1:0>3}.h5'.format(*pdo_save_path,iens)\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "        ModelCheckpoint(model_save_name, monitor='val_loss', save_best_only=True, verbose=0,write_grads=True),\n",
    "    ]\n",
    "    history  = m1.fit([train_X1], train_Y1, epochs=1000, batch_size=8, shuffle=True, verbose=1,\n",
    "                      validation_split=0.1, callbacks=callbacks)\n",
    "    \n",
    "    loss     = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs   = range(len(loss))\n",
    "    np.savez(\"{0}traineval_ens{1:0>3}.npz\".format(*pdo_save_path,iens), loss=loss,val_loss=val_loss)\n",
    "    \n",
    "    train_Y1_pred = m1.predict(train_X1)\n",
    "    test_Y1_pred  = m1.predict(test_X1)\n",
    "    # evaluation\n",
    "    osc_r2[iens,0]  = r2_score(train_Y1, train_Y1_pred)\n",
    "    osc_r2[iens,1]  = r2_score(test_Y1,  test_Y1_pred)\n",
    "    osc_mse[iens,0] = mean_squared_error(train_Y1, train_Y1_pred)\n",
    "    osc_mse[iens,1] = mean_squared_error(test_Y1,  test_Y1_pred)\n",
    "    # prediction\n",
    "    fut_Y_pred      = m1.predict(XX1_fut)\n",
    "    \n",
    "    # training for trend########################################\n",
    "    # train\n",
    "    train_X2, test_X2, train_Y2, test_Y2=train_test_split(Input_Train_Scaler_trend, Output_Train_Scaler_trend, test_size=0.2)\n",
    "    m2            = train_cnn(train_X2,train_Y2,temp_save_path,iens) \n",
    "    train_Y2_pred = m2.predict(train_X2)\n",
    "    test_Y2_pred  = m2.predict(test_X2)\n",
    "    # evaluation\n",
    "    trend_r2[iens,0]  = r2_score(train_Y2, train_Y2_pred)\n",
    "    trend_r2[iens,1]  = r2_score(test_Y2,  test_Y2_pred)\n",
    "    trend_mse[iens,0] = mean_squared_error(train_Y2, train_Y2_pred)\n",
    "    trend_mse[iens,1] = mean_squared_error(test_Y2,  test_Y2_pred)\n",
    "    # prediction\n",
    "    fut126_Y_pred = m2.predict(Input_Forecast126_Scaler)\n",
    "    fut245_Y_pred = m2.predict(Input_Forecast245_Scaler)\n",
    "    fut370_Y_pred = m2.predict(Input_Forecast370_Scaler)\n",
    "    fut585_Y_pred = m2.predict(Input_Forecast585_Scaler)\n",
    "    \n",
    "    ################################################################\n",
    "    # save output\n",
    "    save_train  = vstack((yrf[TIME_STEPS:-1],fut_Y_pred[:,0],\n",
    "                          fut126_Y_pred[TIME_STEPS:-1,0],\n",
    "                          fut245_Y_pred[TIME_STEPS:-1,0],\n",
    "                          fut370_Y_pred[TIME_STEPS:-1,0],\n",
    "                          fut585_Y_pred[TIME_STEPS:-1,0]))\n",
    "    save_train = save_train.transpose()\n",
    "    sf = pd.DataFrame(save_train)\n",
    "    sf.to_csv(\"./cnn_lstm/model_enspred{0:0>3}_scalar.csv\".format(iens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras_env)",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
